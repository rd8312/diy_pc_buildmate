{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 套件與資料載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc_diy/.conda/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from chains import classification_chain_second\n",
    "\n",
    "from chains import general_chain\n",
    "from chains import evaluation_chain, score_chain\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from chains import db_chain\n",
    "from prompt import SQL_PROMPT\n",
    "import ast\n",
    "from prompt import *\n",
    "\n",
    "from chains import retrieve_2_chain\n",
    "from chains import recommend_eva_chain, score_chain\n",
    "from units import find_item, find_index\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   import argparse\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='')\n",
    "#     parser.add_argument('--test_dataset_path', dest='test_dataset_path')\n",
    "#     parser.add_argument('--config_path', dest='config_path')\n",
    "#     parameter_args = parser.parse_args()\n",
    "\n",
    "#     test_dataset_path = parameter_args.test_dataset_path\n",
    "#     config_path = parameter_args.config_path\n",
    "\n",
    "test_dataset_path = 'data/test_dataset.json'\n",
    "config_path = 'config.json'\n",
    "\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_set = json.load(f)\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "recommend_path = config['RAG_path']\n",
    "    \n",
    "with open(recommend_path, 'r') as f:\n",
    "    recommend_data = json.load(f)\n",
    "    \n",
    "only_for_classification = test_set['only_for_classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型預測: 100%|██████████| 15/15 [00:46<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(recommend_path, 'r') as f:\n",
    "    recommend_data = json.load(f)\n",
    "\n",
    "classifier_error_examples = []\n",
    "\n",
    "for data in tqdm(test_set['data_list'], desc='模型預測'):\n",
    "    \n",
    "    question_class = data['class']\n",
    "    question = data['question']\n",
    "    \n",
    "    predict_class = classification_chain_second.invoke({\"question\": f\"{question}\"})\n",
    "    predict_class = re.findall('\\d', predict_class)[0]\n",
    "    \n",
    "    data['predict_class'] = predict_class\n",
    "    \n",
    "    if only_for_classification:\n",
    "        pass\n",
    "    \n",
    "    elif question_class == predict_class:\n",
    "            \n",
    "        if predict_class == '1':\n",
    "            \n",
    "            predict = general_chain.invoke({\"question\": f\"{question}\"})                \n",
    "        \n",
    "        if predict_class == '2':\n",
    "            \n",
    "            return_data = db_chain.invoke(question)\n",
    "            if return_data['result']:\n",
    "                predict = ast.literal_eval(return_data['result'])[0][0]\n",
    "            else:\n",
    "                predict = ''\n",
    "            \n",
    "        if predict_class == '4':\n",
    "            \n",
    "            message = question + RECOMMEND_PROMPT\n",
    "            result = retrieve_2_chain.invoke({\"question\": message, \"chat_history\": []})\n",
    "            recommend = result['answer']\n",
    "\n",
    "            recommend_index = find_index(recommend)\n",
    "            if recommend_index == None: raise '找不到序號+數字'\n",
    "            recommend_item = find_item(recommend_data, recommend_index)\n",
    "            if recommend_item == False:raise 'JSON中沒有該序號的資料'\n",
    "            predict = recommend_item['items']\n",
    "            \n",
    "            data['recommend'] = recommend\n",
    "        data['predict'] = predict\n",
    "    else:\n",
    "        classifier_error_examples.append(question)\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型評分: 100%|██████████| 15/15 [00:36<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "classifier_error_examples = []\n",
    "\n",
    "for data in tqdm(test_set['data_list'], desc='模型評分'):\n",
    "    \n",
    "    question_class = data['class']\n",
    "    predict_class = data['predict_class']\n",
    "    \n",
    "    if only_for_classification:\n",
    "        pass\n",
    "    \n",
    "    elif question_class == predict_class:\n",
    "        predict = data['predict']\n",
    "        question = data['question']\n",
    "            \n",
    "        if predict_class == '1':\n",
    "            \n",
    "            evaluation = evaluation_chain.invoke({\"question\": f\"{question}\", \"answer\": f\"{predict}\"})\n",
    "            evaluation = evaluation.replace('輸出:', '').replace(' ', '')\n",
    "            \n",
    "            score = score_chain.invoke({\"question\": f\"{question}\", \"answer\": f\"{predict}\", \"evaluation\": f\"{evaluation}\"})\n",
    "            score = re.findall(r'\\d+', score)[0]\n",
    "                \n",
    "        if predict_class == '2':\n",
    "            \n",
    "            answer = data['answer']\n",
    "            answer_type = data['answer_type']\n",
    "\n",
    "            # 更複雜的計分規則，可以來討論，例如型號與名稱，可以接受模糊比對\n",
    "            if answer_type == 'int':\n",
    "                if int(answer) == int(predict):\n",
    "                    score = 10\n",
    "                else:\n",
    "                    score = 0\n",
    "            if answer_type == 'str':\n",
    "                if answer in predict:\n",
    "                    score = 10\n",
    "                else:\n",
    "                    score = 0\n",
    "            if answer_type == 'error':\n",
    "                if answer == '':\n",
    "                    score = 10\n",
    "                else:\n",
    "                    score = 0\n",
    "            \n",
    "            evaluation = ''\n",
    "            \n",
    "        if predict_class == '4':\n",
    "            \n",
    "            evaluation = recommend_eva_chain.invoke({\"question\": f\"{question}\", \"predict\": f\"{predict}\"})\n",
    "            score = score_chain.invoke({\"question\": f\"{question}\", \"answer\": f\"{predict}\", \"evaluation\": f\"{evaluation}\"})\n",
    "            score = re.findall(r'\\d+', score)[0]\n",
    "            \n",
    "        data['evaluation'] = evaluation\n",
    "        data['score'] = score\n",
    "        \n",
    "    else:\n",
    "        error_example = {}\n",
    "        data['score'] = 0\n",
    "        classifier_error_examples.append(data)\n",
    "        \n",
    "if classifier_error_examples and only_for_classification: print(classifier_error_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分數計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Score: 10.0\n",
      "General Score: 9.0 \n",
      "SQL Score: 10.0 \n",
      "Retrieve Score: 8.2 \n"
     ]
    }
   ],
   "source": [
    "classifier_predict = 0\n",
    "\n",
    "class_1_scores = []\n",
    "class_2_scores = []\n",
    "class_4_scores = []\n",
    "\n",
    "for data in test_set['data_list']:\n",
    "    \n",
    "    data_class = data['class'] \n",
    "    predict_class = data['predict_class'] \n",
    "    \n",
    "    if predict_class == data_class:\n",
    "        classifier_predict += 1\n",
    "        \n",
    "        if only_for_classification:\n",
    "            pass\n",
    "        else:\n",
    "            data_score = data['score'] \n",
    "            \n",
    "            if data_class == '1':\n",
    "                class_1_scores.append(data_score)\n",
    "            elif data_class == '2':\n",
    "                class_2_scores.append(data_score)\n",
    "            elif data_class == '4':\n",
    "                class_4_scores.append(data_score)\n",
    "        \n",
    "classifier_accuracy = int(classifier_predict / len(test_set['data_list']) *100)\n",
    "classifier_average = float(classifier_predict / len(test_set['data_list']) * 10)\n",
    "\n",
    "test_set['Classifier Accuracy'] = classifier_accuracy\n",
    "test_set['score_data']['Classifier'] = classifier_average\n",
    "print(f'Classifier Score: {classifier_average}')\n",
    "\n",
    "if class_1_scores:\n",
    "    class_1_average = sum([int(num) for num in class_1_scores]) / len(class_1_scores)\n",
    "    test_set['score_data']['General'] = class_1_average\n",
    "    print(f'General Score: {class_1_average} ')\n",
    "    \n",
    "if class_2_scores:\n",
    "    class_2_average = sum([int(num) for num in class_2_scores]) / len(class_2_scores)\n",
    "    test_set['score_data']['SQL'] = class_2_average\n",
    "    print(f'SQL Score: {class_2_average} ')\n",
    "    \n",
    "if class_4_scores:\n",
    "    class_4_average = sum([int(num) for num in class_4_scores]) / len(class_4_scores)\n",
    "    test_set['score_data']['Retrieve'] = class_4_average\n",
    "    print(f'Retrieve Score: {class_4_average} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 紀錄儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report path : report/2024-03-05 11-21-34.json\n"
     ]
    }
   ],
   "source": [
    "from units import get_time_text\n",
    "\n",
    "_, evaluation_datetime = get_time_text()\n",
    "test_set['Evaluation datetime'] = evaluation_datetime\n",
    "evaluation_json_path = f'report/{evaluation_datetime}.json'\n",
    "\n",
    "with open(evaluation_json_path, 'w') as rm:\n",
    "    json.dump(test_set, rm, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f'Report path : {evaluation_json_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
