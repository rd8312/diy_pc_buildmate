{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for PTT\n",
    "import json\n",
    "from datetime import datetime\n",
    "ptt_recommend_list = [\"./ptt_RAG_10000_all.jsonl\"]\n",
    "\n",
    "\n",
    "\n",
    "def parse_json_if_possible(content):\n",
    "    \n",
    "    if isinstance(content, dict):\n",
    "        return content\n",
    "    \n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        return content\n",
    "\n",
    "documents = []\n",
    "for recommend_path in ptt_recommend_list:    \n",
    "    \n",
    "    with open(recommend_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "           \n",
    "            json_obj = json.loads(line)\n",
    "            \n",
    "            \n",
    "            combined_content = \"\"\n",
    "            for msg in json_obj[\"messages\"]:\n",
    "                print (msg)\n",
    "                \n",
    "                content = parse_json_if_possible(msg[\"content\"])\n",
    "                \n",
    "                if isinstance(content, dict):\n",
    "                   \n",
    "                    content_str = \", \".join([f\"{k}: {v}\" for k, v in content.items()])\n",
    "                    combined_content += content_str + \"\\n\"\n",
    "                elif msg['role'] == 'date':\n",
    "                    original_format = \"%a %b %d %H:%M:%S %Y\"\n",
    "                    \n",
    "                    datetime_obj = datetime.strptime(content, original_format)\n",
    "\n",
    "                    \n",
    "                    new_format = \"%Y-%m-%d %H:%M\"\n",
    "                    \n",
    "                    new_date_str = datetime_obj.strftime(new_format)\n",
    "\n",
    "                    combined_content += \"文章日期：\" + new_date_str  + \"\\n\"\n",
    "                else:\n",
    "                    \n",
    "                    combined_content += content + \"\\n\"\n",
    "                \n",
    "            # 添加到 documents 列表\n",
    "            combined_content += \"文章來源: https://www.ptt.cc/bbs/PC_Shopping/\\n\"\n",
    "            documents.append({\"text\": combined_content.strip()})\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile01\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def parse_json(content):\n",
    "    \n",
    "    json_string = re.search(r'```json\\n(.+)```', content, re.DOTALL)\n",
    "    if json_string:\n",
    "        extracted_json_string = json_string.group(1)\n",
    "        # Replace the non-breaking space character with a regular space for JSON parsing\n",
    "        cleaned_json_string = extracted_json_string.replace('\\xa0', ' ')\n",
    "        print (cleaned_json_string)\n",
    "        # Convert the cleaned string back to JSON format\n",
    "        try:\n",
    "            json_data = json.loads(f'[{cleaned_json_string}]')\n",
    "            item_length = len(json_data)\n",
    "            items = []\n",
    "            for item in json_data:\n",
    "                #print (item['name'], item['quantity'])\n",
    "                pass\n",
    "        except Exception:\n",
    "            json_data = None\n",
    "\n",
    "    else:\n",
    "        json_data = None\n",
    "    return json_data\n",
    "    \n",
    "def recommend_to_text(recommend):\n",
    "    text = \"\"\n",
    "    for r in recommend:\n",
    "        text += '\\n'.join([ f\"- {i['name']}, ${i['subtotal']}\" for i in r ])\n",
    "\n",
    "    \n",
    "            \n",
    "    return text\n",
    "            \n",
    "def html_to_text(html_content):\n",
    "\n",
    "   soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "   # Extract\n",
    "   text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "   #print(text_content)\n",
    "   return text_content\n",
    "def parse_thread(discussion_thread):\n",
    "   \n",
    "    requirement = discussion_thread['content']\n",
    "    article_list = []\n",
    "    requirement_json_data_list = []\n",
    "    requirement_line = \"\"\n",
    "    for line in requirement:\n",
    "        \n",
    "        json_data = parse_json(line)\n",
    "        if json_data:\n",
    "            print ('menu found!!!')\n",
    "            requirement_json_data_list.append(json_data)\n",
    "            print ('add to list:', json_data)\n",
    "            item_flag = True\n",
    "        else:\n",
    "            requirement_line += line\n",
    "    \n",
    "    i = 0\n",
    "    # iterate each reply\n",
    "    for r in discussion_thread['replayDetail']:\n",
    "        article = {}\n",
    "        article['requirement'] = f\"{discussion_thread['title']}\\n{html_to_text(requirement_line)}\"\n",
    "        article['requirement_menu'] = requirement_json_data_list\n",
    "        article['date'] = discussion_thread['createDate']\n",
    "        article['source'] = 'https://www.mobile01.com/topiclist.php?f=174'\n",
    "        print ('*'*10, \"reply\", i)\n",
    "        date = r['replayDate']\n",
    "        content = r['content']\n",
    "        item_flag = False\n",
    "        content_line = \"\"\n",
    "        json_data_list = []\n",
    "        for line in content:\n",
    "            #print ('cntent_line:', line)\n",
    "            json_data = parse_json(line)\n",
    "            if json_data:\n",
    "                print ('menu found!!!')\n",
    "                \n",
    "                json_data_list.append(json_data[0])\n",
    "                print ('add to list:', json_data[0])\n",
    "                #print (json_data)\n",
    "                item_flag = True\n",
    "            else:\n",
    "                content_line += line\n",
    "        if item_flag:\n",
    "            print ('content line:', content_line)\n",
    "            print (json_data_list)\n",
    "            article['recommend_reason'] = html_to_text(content_line)\n",
    "            article['recommend_menu'] = recommend_to_text(json_data_list)\n",
    "            article_list.append(article)\n",
    "        print ('\\n')\n",
    "        i = i + 1\n",
    "    return article_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load Mobile01 Data\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "directory = Path('./mobile01_3/')\n",
    "def find_json_files(directory):\n",
    "    \n",
    "    path = Path(directory)\n",
    "    \n",
    "    json_files = path.rglob('*.json')\n",
    "    \n",
    "    \n",
    "    return [str(file) for file in json_files]\n",
    "\n",
    "json_files = find_json_files(directory)\n",
    "\n",
    "print (json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# the file of the OCR results\n",
    "thread_list = []\n",
    "for f in json_files:\n",
    "    with open(f, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print (f)\n",
    "        for discussion_thread in data:\n",
    "            recommend_list = parse_thread(discussion_thread)\n",
    "            thread_list.append(recommend_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mobile01_text(mobile01_data):\n",
    "    #pprint(mobile01_thread)\n",
    "    recommend_list = []\n",
    "    for mobile01_thread in mobile01_data:\n",
    "        text = f\"\"\"需求：{mobile01_thread['requirement']}\\n網友推薦原因:{mobile01_thread['recommend_reason']}\\n網友推薦組合：\\n{mobile01_thread['recommend_menu']}\\n文章日期：{mobile01_thread[\n",
    "            'date']}\\n文章來源：{mobile01_thread['source']}\n",
    "        \"\"\"\n",
    "        print (text)\n",
    "        recommend_list.append({'text':text})\n",
    "    return recommend_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile01_documents = []\n",
    "for t in thread_list:\n",
    "    recommend_list = convert_mobile01_text(t)\n",
    "    mobile01_documents.extend(recommend_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫入整合的檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = 'combined_ptt_mobile01_documents_all_ptt_smaller_v2.jsonl'\n",
    "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "    \n",
    "    for document in documents:\n",
    "        \n",
    "        json_str = json.dumps(document, ensure_ascii=False)\n",
    "        f.write(json_str + '\\n')\n",
    "    \n",
    "    for document in mobile01_documents[0:1500]:\n",
    "        \n",
    "        json_str = json.dumps(document, ensure_ascii=False)\n",
    "        f.write(json_str + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_ptt_mobile01_documents_all_ptt_smaller_v2.jsonl\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "import json\n",
    "from chromadb import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "output_json_path = 'combined_ptt_mobile01_documents_all_ptt_smaller_v2.jsonl'\n",
    "print (output_json_path)\n",
    "loader = JSONLoader(\n",
    "    file_path = output_json_path,\n",
    "    jq_schema = '.text',\n",
    "    \n",
    "    json_lines=True, text_content=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_load = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for d in documents_load:\n",
    "    documents.append(d.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tqdm\n",
    "client = OpenAI()\n",
    "\n",
    "def embedding_model(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding  # 1536-dim list\n",
    "def process_documents_and_add_to_chroma(documents, collection, batch_size=100, delay=60):\n",
    "    total_docs = len(documents)\n",
    "    embedding_data = []\n",
    "    for start_idx in range(0, total_docs, batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch = documents[start_idx:end_idx]\n",
    "        \n",
    "        document_ids = [f\"doc_{i}\" for i in range(start_idx, end_idx)]  # 生成文檔ID列表\n",
    "        print (document_ids[:5])\n",
    "        \n",
    "        for doc_id, text in zip(document_ids, batch):\n",
    "            embedding = embedding_model(text)  # obtain embedding\n",
    "            embedding_data.append(embedding)  \n",
    "        \n",
    "        if end_idx < total_docs:\n",
    "            print(f\"Batch {start_idx//batch_size + 1} completed. Waiting for {delay} seconds...\")\n",
    "            pass #time.sleep(delay)\n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc_0', 'doc_1', 'doc_2', 'doc_3', 'doc_4']\n",
      "Batch 1 completed. Waiting for 10 seconds...\n",
      "['doc_100', 'doc_101', 'doc_102', 'doc_103', 'doc_104']\n",
      "Batch 2 completed. Waiting for 10 seconds...\n",
      "['doc_200', 'doc_201', 'doc_202', 'doc_203', 'doc_204']\n",
      "Batch 3 completed. Waiting for 10 seconds...\n",
      "['doc_300', 'doc_301', 'doc_302', 'doc_303', 'doc_304']\n",
      "Batch 4 completed. Waiting for 10 seconds...\n",
      "['doc_400', 'doc_401', 'doc_402', 'doc_403', 'doc_404']\n",
      "Batch 5 completed. Waiting for 10 seconds...\n",
      "['doc_500', 'doc_501', 'doc_502', 'doc_503', 'doc_504']\n",
      "Batch 6 completed. Waiting for 10 seconds...\n",
      "['doc_600', 'doc_601', 'doc_602', 'doc_603', 'doc_604']\n",
      "Batch 7 completed. Waiting for 10 seconds...\n",
      "['doc_700', 'doc_701', 'doc_702', 'doc_703', 'doc_704']\n",
      "Batch 8 completed. Waiting for 10 seconds...\n",
      "['doc_800', 'doc_801', 'doc_802', 'doc_803', 'doc_804']\n",
      "Batch 9 completed. Waiting for 10 seconds...\n",
      "['doc_900', 'doc_901', 'doc_902', 'doc_903', 'doc_904']\n",
      "Batch 10 completed. Waiting for 10 seconds...\n",
      "['doc_1000', 'doc_1001', 'doc_1002', 'doc_1003', 'doc_1004']\n",
      "Batch 11 completed. Waiting for 10 seconds...\n",
      "['doc_1100', 'doc_1101', 'doc_1102', 'doc_1103', 'doc_1104']\n",
      "Batch 12 completed. Waiting for 10 seconds...\n",
      "['doc_1200', 'doc_1201', 'doc_1202', 'doc_1203', 'doc_1204']\n",
      "Batch 13 completed. Waiting for 10 seconds...\n",
      "['doc_1300', 'doc_1301', 'doc_1302', 'doc_1303', 'doc_1304']\n",
      "Batch 14 completed. Waiting for 10 seconds...\n",
      "['doc_1400', 'doc_1401', 'doc_1402', 'doc_1403', 'doc_1404']\n",
      "Batch 15 completed. Waiting for 10 seconds...\n",
      "['doc_1500', 'doc_1501', 'doc_1502', 'doc_1503', 'doc_1504']\n",
      "Batch 16 completed. Waiting for 10 seconds...\n",
      "['doc_1600', 'doc_1601', 'doc_1602', 'doc_1603', 'doc_1604']\n",
      "Batch 17 completed. Waiting for 10 seconds...\n",
      "['doc_1700', 'doc_1701', 'doc_1702', 'doc_1703', 'doc_1704']\n",
      "Batch 18 completed. Waiting for 10 seconds...\n",
      "['doc_1800', 'doc_1801', 'doc_1802', 'doc_1803', 'doc_1804']\n",
      "Batch 19 completed. Waiting for 10 seconds...\n",
      "['doc_1900', 'doc_1901', 'doc_1902', 'doc_1903', 'doc_1904']\n",
      "Batch 20 completed. Waiting for 10 seconds...\n",
      "['doc_2000', 'doc_2001', 'doc_2002', 'doc_2003', 'doc_2004']\n",
      "Batch 21 completed. Waiting for 10 seconds...\n",
      "['doc_2100', 'doc_2101', 'doc_2102', 'doc_2103', 'doc_2104']\n",
      "Batch 22 completed. Waiting for 10 seconds...\n",
      "['doc_2200', 'doc_2201', 'doc_2202', 'doc_2203', 'doc_2204']\n",
      "Batch 23 completed. Waiting for 10 seconds...\n",
      "['doc_2300', 'doc_2301', 'doc_2302', 'doc_2303', 'doc_2304']\n"
     ]
    }
   ],
   "source": [
    "embedding_data = process_documents_and_add_to_chroma(documents, None, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./embedding_data.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "with open('embedding_data.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    # embeddings shape: (len(data_list), 1536)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"預算有8萬，Deep learning /AI 主機，要買什麼零件\"\n",
    "query_embedding = embedding_model(query)\n",
    "distances, indices = index.search(np.array([query_embedding], dtype=np.float32), 20)\n",
    "results = [documents[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "query = \"預算有8萬，想組 deep learning / AI 主機，要買什麼零件？\"\n",
    "user_question = query\n",
    "\n",
    "    \n",
    "# 模拟的模型和温度参数，您需要替换为实际的模型名称和参数\n",
    "# 系統設定\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "temperature = 0\n",
    "\n",
    "# 模拟的ChatOpenAI初始化，这里需要替换为您实际的初始化逻辑\n",
    "chat_model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "# 請提供每個零件的網友的評價，如果有評價不好的，要把不好的評價附上。零組件的清單裡如果有友情贊助的品項，要移除掉\"友情\"的詞\n",
    "# 构建聊天模板\n",
    "now = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8)))\n",
    "today_str = f\"- 今天日期：{now.strftime('%Y/%m/%d')}\"\n",
    "template = \"\"\"你是一個電腦組裝助手，請依照使用者的'預算'與'需求'，以及網路論壇找到的組裝清單與網友評價，挑選網友評價好的品項給使用者。\n",
    "Perform the following actions:\n",
    "- 1. 請寫出使用者的 預算 與 需求。\n",
    "- 2. 請寫出你找到的清單數量。\n",
    "- 3. 依序列出你找到的每一張清單的內容。依以下子步驟 (3.1~3.5) 寫下每一張清單的內容。\n",
    "- 3.1. 網路找到的組裝清單通常有註名是依什麼預算挑出零件的，所以你必須先比對組裝清單裡的預算與使用者的預算之間的差額，請寫下清單的預算與清單的預算和使用者預算的差額，若沒有清單預算則計算清單總價，並寫下清單總價與使用者預算的差額！\n",
    "- 3.2. 清單總價或是清單裡的預算和使用者的預算差額的絕對值要在30000元內才算是符合使用者的預算.\n",
    "- 3.3. 寫出清單裡的每一個零件時，一定要附上每個零件的網友的評價與網友推薦原因，如果有評價不好的，一定要把不好的評價附上給使用者參考！\n",
    "- 3.4. 請註名清單來自 PTT or Mobile01！\n",
    "- 3.5. 請註名文章日期，請註名今天的日期！\n",
    "- 4. 總結哪一張清單的總價與使用者的預算最接近，網友評價好的，且文章日期離今天日期在3個月內的，並予以推薦.\n",
    "網路論壇找到的組裝清單與網友評價: ```{background_context}```\n",
    "Hard requirements that must be followed:\n",
    "- 千萬不要列出 step 的數字！\n",
    "\n",
    "\"\"\"\n",
    "template += today_str\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{user_question}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "query_embedding = embedding_model(query)\n",
    "distances, indices = index.search(np.array([query_embedding], dtype=np.float32), 10)\n",
    "docs_searched = [documents[i] for i in indices[0]]\n",
    "\n",
    "response = chat_model(chat_prompt.format_messages(background_context=docs_searched, user_question=user_question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
