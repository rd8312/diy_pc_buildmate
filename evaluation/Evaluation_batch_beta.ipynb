{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 套件與資料載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# sys.path.append('.')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import ast\n",
    "from units import find_item, find_index\n",
    "\n",
    "import json\n",
    "import os\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   import argparse\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='')\n",
    "#     parser.add_argument('--config_path', dest='config_path')\n",
    "#     parameter_args = parser.parse_args()\n",
    "\n",
    "#     config_path = parameter_args.config_path\n",
    "\n",
    "config_path = '../config/evaluation_config.json'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "os.environ['OPENAI_API_KEY'] = config['OpenAI_api_key']\n",
    "\n",
    "test_dataset_path = config['test_dataset_path']\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_set = json.load(f)\n",
    "\n",
    "recommend_path = config['GPTs']['Retrieve']['database_path']\n",
    "with open(recommend_path, 'r') as f:\n",
    "    recommend_data = json.load(f)\n",
    "    \n",
    "only_for_classification = test_set['only_for_classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import make_chain, make_memory, Chain_manager\n",
    "\n",
    "SQL_PROMPT = config['GPTs']['SQL']['prompt']\n",
    "RECOMMEND_PROMPT = config['GPTs']['Retrieve']['prompt']\n",
    "\n",
    "chain_manager = Chain_manager(config)\n",
    "classifier_chain, _ = chain_manager.make_chain('Classifier')\n",
    "general_chain, _ = chain_manager.make_chain('Netizen') # Netizen General\n",
    "db_chain, _ = chain_manager.make_chain('SQL')\n",
    "retrieve_chain, _ = chain_manager.make_chain('Retrieve')\n",
    "consultant_chain, memory = chain_manager.make_chain('Consultant')\n",
    "\n",
    "comment_chain, _ = chain_manager.make_chain('Comment')\n",
    "score_chain, _ = chain_manager.make_chain('General_user_score')\n",
    "recommend_eva_chain, _ = chain_manager.make_chain('Recommend_evaluation')\n",
    "consultant_comment_chain, _ = chain_manager.make_chain('Consultant_Comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to batch\n",
    "batch_size = 10\n",
    "\n",
    "questions_group = []\n",
    "accumulate = 0\n",
    "group_index = 1\n",
    "\n",
    "group = {'data_list':[]}\n",
    "collect_class = test_set['data_list'][0]['class']\n",
    "\n",
    "for data in test_set['data_list']:\n",
    "    \n",
    "    data_class = data['class']\n",
    "    if data_class != collect_class or accumulate == batch_size:\n",
    "        \n",
    "        group['class'] = collect_class\n",
    "        group['group_index'] = group_index\n",
    "        questions_group.append(group)\n",
    "        collect_class = data_class\n",
    "        \n",
    "        group = {'data_list':[]}\n",
    "        accumulate = 0\n",
    "        group_index += 1\n",
    "    \n",
    "    question = data['question']\n",
    "    group['data_list'].append({\"question\": f\"{question}\"})\n",
    "    \n",
    "    accumulate += 1   \n",
    "    \n",
    "group['class'] = collect_class\n",
    "group['group_index'] = group_index\n",
    "questions_group.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型預測:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型預測: 100%|██████████| 10/10 [01:43<00:00, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index search time: 103.93152523040771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    \n",
    "    data_index = 0\n",
    "    for group in tqdm(questions_group, desc='模型預測'):\n",
    "        \n",
    "        predict_list = general_chain.batch(group['data_list'])    \n",
    "        predict_class_list = classifier_chain.batch(group['data_list']) \n",
    "        accumulate = 0\n",
    "        \n",
    "        for predict_class, predict in zip(predict_class_list, predict_list):\n",
    "            \n",
    "            group['data_list'][accumulate]['answer'] = predict\n",
    "            \n",
    "            predict_class = re.findall('\\d', predict_class)[0]\n",
    "            test_set['data_list'][data_index]['predict_class'] = predict_class\n",
    "            \n",
    "            data_index += 1\n",
    "            accumulate += 1\n",
    "            \n",
    "time2 = time.time()\n",
    "process_time = time2 - time1\n",
    "print('index search time:', process_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型預測:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型預測: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "for group in tqdm(questions_group, desc='模型預測'):\n",
    "    \n",
    "    accumulate = 0\n",
    "    score_list = score_chain.batch(group['data_list'])\n",
    "    for score in score_list:\n",
    "        score = re.findall(r'\\d+', score)[0]\n",
    "        \n",
    "        group['data_list'][accumulate]['score'] = float(score)\n",
    "        accumulate += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier score: 10.0\n",
      "General score: 7.96\n",
      "Total response count: 14436\n",
      "Cost: 0.418304500000001\n",
      "Token: 80307\n",
      "Inference time 103.93152523040771\n"
     ]
    }
   ],
   "source": [
    "total_classifier_score = 0\n",
    "total_general_score = 0\n",
    "data_num = 0\n",
    "data_index = 0\n",
    "\n",
    "for group in questions_group:\n",
    "    \n",
    "    for data in group['data_list']:\n",
    "        \n",
    "        total_general_score += data['score']\n",
    "        data_num += 1\n",
    "        \n",
    "        test_set['data_list'][data_index]['answer'] = data['answer']\n",
    "        test_set['data_list'][data_index]['score'] = data['score']\n",
    "        \n",
    "        predict_class = test_set['data_list'][0]['predict_class']\n",
    "        data_class = test_set['data_list'][0]['class']\n",
    "        \n",
    "        if predict_class == data_class:\n",
    "            total_classifier_score += 10\n",
    "        \n",
    "        data_index += 1\n",
    "classifier_average = float(total_classifier_score/ data_num)\n",
    "general_average = float(total_general_score/ data_num)\n",
    "print(f'Classifier score: {classifier_average}')\n",
    "print(f'General score: {general_average}')\n",
    "\n",
    "total_response_count = 0\n",
    "for data in test_set['data_list']:\n",
    "    total_response_count += len(data['answer'])\n",
    "print(f'Response count: {total_response_count}')\n",
    "\n",
    "test_set['Response count'] = total_response_count\n",
    "test_set['token'] = cb.total_tokens\n",
    "test_set['cost'] = cb.total_cost\n",
    "test_set['Inference time'] = process_time\n",
    "print(f'Cost: {round(cb.total_cost, 15)}')\n",
    "print(f'Token: {cb.total_tokens}')\n",
    "print(f'Inference time', process_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # General\n",
    "# Classifier score: 10.0\n",
    "# General score: 8.01\n",
    "# Response count: 25097\n",
    "# Cost: 0.167917\n",
    "# Token: 101056\n",
    "# Inference time 120.15399718284607\n",
    "\n",
    "# # Netizen\n",
    "# Classifier score: 10.0\n",
    "# General score: 7.96\n",
    "# Response count: 14436\n",
    "# Cost: 0.418304500000001\n",
    "# Token: 80307\n",
    "# Inference time 103.93152523040771\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 紀錄儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report path : ../report/2024-03-20 13-32-21.json\n"
     ]
    }
   ],
   "source": [
    "from units import get_time_text\n",
    "\n",
    "_, evaluation_datetime = get_time_text()\n",
    "test_set['Evaluation datetime'] = evaluation_datetime\n",
    "report_path = config['report_path']\n",
    "evaluation_json_path = f'{report_path}/{evaluation_datetime}.json'\n",
    "\n",
    "with open(evaluation_json_path, 'w') as rm:\n",
    "    json.dump(test_set, rm, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f'Report path : {evaluation_json_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
